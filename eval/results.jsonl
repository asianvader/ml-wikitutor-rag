{"id": "q1", "question": "What is the difference between supervised and unsupervised learning?", "answer": "The difference between supervised and unsupervised learning lies in the presence of labeled data. \n\n- **Supervised learning** uses labeled data, meaning each input is paired with a correct output. The algorithm learns to map inputs to outputs based on these examples, aiming to predict outputs for new data. It is commonly used for classification and regression tasks [3][4].\n\n- **Unsupervised learning** works with unlabeled data, where the algorithm identifies patterns or structures without explicit output labels. It includes tasks like clustering, dimensionality reduction, and density estimation, learning from the data's inherent similarities or features [1][2][4].", "confidence": {"label": "High", "value": 0.85, "best": 0.30011552572250366, "worst": 0.4135504961013794, "spread": 0.11343497037887573}, "sources": [{"n": 1, "title": "Unsupervised learning", "url": "https://en.wikipedia.org/?curid=233497", "score": 0.30011552572250366, "chunk_id": "0", "preview": "Unsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data. Other frameworks in t"}, {"n": 2, "title": "Machine learning", "url": "https://en.wikipedia.org/?curid=233488", "score": 0.3906710743904114, "chunk_id": "9", "preview": "on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regre"}, {"n": 3, "title": "Supervised learning", "url": "https://en.wikipedia.org/?curid=20926", "score": 0.40687596797943115, "chunk_id": "0", "preview": "In machine learning, supervised learning (SL) is a type of machine learning paradigm where an algorithm learns to map input data to a specific output based on example input-output "}, {"n": 4, "title": "Neural network (machine learning)", "url": "https://en.wikipedia.org/?curid=21523", "score": 0.4135504961013794, "chunk_id": "14", "preview": "and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervise"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:04.791268Z"}
{"id": "q2", "question": "What is PCA used for?", "answer": "PCA is used to reduce large amounts of data into smaller, more manageable variables for easier analysis. It is applied in various fields such as market research to develop customer satisfaction scores and identify latent variables underlying consumer attitudes, and in quantitative finance for financial risk management, portfolio optimization, and modeling interest rate movements by explaining correlated instruments in the yield curve. It also helps in calculating value at risk (VaR) through Monte Carlo simulations [1].", "confidence": {"label": "High", "value": 0.85, "best": 0.3112470507621765, "worst": 0.3112470507621765, "spread": 0.0}, "sources": [{"n": 1, "title": "Principal component analysis", "url": "https://en.wikipedia.org/?curid=76340", "score": 0.3112470507621765, "chunk_id": "54", "preview": ". In August 2022, the molecular biologist Eran Elhaik published a theoretical paper in Scientific Reports analyzing 12 PCA applications. He concluded that it was easy to manipulate"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:08.263467Z"}
{"id": "q3", "question": "What is overfitting?", "answer": "Overfitting is when a learning algorithm models the training data too closely by using a model that is more complex than necessary, such as having too many adjustable parameters. This causes the model to perform well on training data but poorly on unseen validation data because it captures noise or random features that do not generalize. Overfitting violates Occam's razor, which favors simpler models unless complexity significantly improves fit. It often occurs when training is too long or training data is scarce, leading to a model that fits specific random details rather than the underlying pattern [1].", "confidence": {"label": "High", "value": 0.65, "best": 0.2663627862930298, "worst": 0.5835440158843994, "spread": 0.31718122959136963}, "sources": [{"n": 1, "title": "Overfitting", "url": "https://en.wikipedia.org/?curid=173332", "score": 0.2663627862930298, "chunk_id": "3", "preview": "Usually, a learning algorithm is trained using some set of \"training data\": exemplary situations for which the desired output is known. The goal is that the algorithm will also per"}, {"n": 2, "title": "Convolutional neural network", "url": "https://en.wikipedia.org/?curid=40409788", "score": 0.5370572805404663, "chunk_id": "29", "preview": "that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the "}, {"n": 3, "title": "Bias–variance tradeoff", "url": "https://en.wikipedia.org/?curid=40678189", "score": 0.5482762455940247, "chunk_id": "2", "preview": " a description of variance and generally can only be improved by selecting information from a comparatively larger space. The option to select many data points over a broad sample "}, {"n": 4, "title": "Gradient boosting", "url": "https://en.wikipedia.org/?curid=26649339", "score": 0.5674879550933838, "chunk_id": "21", "preview": "insensitive to the choice of                         J                 {\\displaystyle J}     in this range,                         J         =         2                 {\\displays"}, {"n": 5, "title": "Supervised learning", "url": "https://en.wikipedia.org/?curid=20926", "score": 0.578019917011261, "chunk_id": "2", "preview": "low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different i"}, {"n": 6, "title": "Regularization (mathematics)", "url": "https://en.wikipedia.org/?curid=2009061", "score": 0.5835440158843994, "chunk_id": "5", "preview": "and their labels                         y                 {\\displaystyle y}     respectively. Typically in learning problems, only a subset of input data and labels are available,"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:11.419768Z"}
{"id": "q4", "question": "What is the difference between precision and recall?", "answer": "Precision is the fraction of relevant instances among the retrieved instances, measuring how many retrieved items are actually relevant. Recall is the fraction of relevant instances that were retrieved, measuring how many relevant items were successfully retrieved from all relevant items available [1].", "confidence": {"label": "High", "value": 0.85, "best": 0.25731974840164185, "worst": 0.44890081882476807, "spread": 0.19158107042312622}, "sources": [{"n": 1, "title": "Precision and recall", "url": "https://en.wikipedia.org/?curid=14343887", "score": 0.25731974840164185, "chunk_id": "0", "preview": "In pattern recognition, information retrieval, object detection and classification (machine learning), precision and recall are performance metrics that apply to data retrieved fro"}, {"n": 2, "title": "F-score", "url": "https://en.wikipedia.org/?curid=4011785", "score": 0.44890081882476807, "chunk_id": "1", "preview": "the precision or the recall is zero.  == Etymology == The name F-measure is believed to be named after a different F function in Van Rijsbergen's book, when introduced to the Fourt"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:14.128337Z"}
{"id": "q5", "question": "What is a confusion matrix?", "answer": "A confusion matrix is a specific table layout used in machine learning to visualize the performance of a classification algorithm. It is a special kind of contingency table with two dimensions: \"actual\" and \"predicted\" classes. Each row represents the instances of an actual class, and each column represents the instances of a predicted class (or vice versa). The diagonal elements represent correctly predicted instances, while off-diagonal elements indicate misclassifications, showing where the system confuses one class for another. This matrix helps in understanding the types of errors made by the classifier and is commonly used in supervised learning problems of statistical classification [1].", "confidence": {"label": "High", "value": 0.65, "best": 0.23663806915283203, "worst": 0.5798144936561584, "spread": 0.3431764245033264}, "sources": [{"n": 1, "title": "Confusion matrix", "url": "https://en.wikipedia.org/?curid=847558", "score": 0.23663806915283203, "chunk_id": "0", "preview": "In machine learning, a confusion matrix, also known as error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised"}, {"n": 2, "title": "Decision tree learning", "url": "https://en.wikipedia.org/?curid=577003", "score": 0.458243727684021, "chunk_id": "6", "preview": ", the total false positives (FP) are subtracted from the total true positives (TP). The resulting number gives an estimate on how many positive examples the feature could correctly"}, {"n": 3, "title": "Sensitivity and specificity", "url": "https://en.wikipedia.org/?curid=5599330", "score": 0.5100805759429932, "chunk_id": "7", "preview": "as a trade off between recall (TPR) and False positive rate (FPR ). Giving them equal weight optimizes informedness = specificity + sensitivity − 1 = TPR − FPR, the magnitude of wh"}, {"n": 4, "title": "Precision and recall", "url": "https://en.wikipedia.org/?curid=14343887", "score": 0.5641750693321228, "chunk_id": "11", "preview": "{\\textstyle r}    , the corresponding precision will decrease, because the denominator increases.  Another metric is the predicted positive condition rate (PPCR), which identifies "}, {"n": 5, "title": "Receiver operating characteristic", "url": "https://en.wikipedia.org/?curid=922505", "score": 0.5702260136604309, "chunk_id": "2", "preview": "tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually "}, {"n": 6, "title": "Cross-validation (statistics)", "url": "https://en.wikipedia.org/?curid=416612", "score": 0.5798144936561584, "chunk_id": "12", "preview": "either predicted correctly or incorrectly. In this situation the misclassification error rate can be used to summarize the fit, although other measures derived from information (e."}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:17.240977Z"}
{"id": "q6", "question": "What is gradient boosting?", "answer": "Gradient boosting is a machine learning technique based on boosting in a functional space, where the target is pseudo-residuals instead of residuals as in traditional boosting. It produces a prediction model as an ensemble of weak prediction models, typically simple decision trees. When decision trees are used as weak learners, the method is called gradient-boosted trees, which often outperform random forests. Gradient boosting builds the model in stages and generalizes other boosting methods by allowing optimization of an arbitrary differentiable loss function [1].", "confidence": {"label": "High", "value": 0.75, "best": 0.2684515714645386, "worst": 0.557181715965271, "spread": 0.2887301445007324}, "sources": [{"n": 1, "title": "Gradient boosting", "url": "https://en.wikipedia.org/?curid=26649339", "score": 0.2684515714645386, "chunk_id": "0", "preview": "Gradient boosting is a machine learning technique based on boosting in a functional space, where the target is pseudo-residuals instead of residuals as in traditional boosting. It "}, {"n": 2, "title": "Backpropagation", "url": "https://en.wikipedia.org/?curid=1360091", "score": 0.557181715965271, "chunk_id": "33", "preview": "To update the weight                                    w                        i             j                                     {\\displaystyle w_{ij}}     using gradient desce"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:20.567640Z"}
{"id": "q7", "question": "Explain L1 vs L2 regularization.", "answer": "L1 and L2 regularization are techniques to prevent overfitting by adding penalty terms to the cost function:\n\n- **L1 regularization (LASSO)** adds a penalty based on the absolute values of the coefficients. This encourages sparsity, meaning it drives some weights to zero, effectively selecting a subset of important features and making the model simpler and more interpretable [1], [2].\n\n- **L2 regularization (ridge regression)** adds a penalty based on the squared values of the coefficients. This encourages smaller, more evenly distributed weights, preventing any single weight from dominating. It tends to keep all features but with reduced magnitude, promoting diffuse weight vectors [1], [2].\n\nL1 leads to sparse models, while L2 leads to smooth, small weights. Both improve generalization by reducing model complexity but in different ways [1], [2].", "confidence": {"label": "Medium", "value": 0.65, "best": 0.38703906536102295, "worst": 0.40713822841644287, "spread": 0.020099163055419922}, "sources": [{"n": 1, "title": "Regularization (mathematics)", "url": "https://en.wikipedia.org/?curid=2009061", "score": 0.38703906536102295, "chunk_id": "1", "preview": "to the training data and the regularization is either the choice of the model or modifications to the algorithm. It is always intended to reduce the generalization error, i.e. the "}, {"n": 2, "title": "Convolutional neural network", "url": "https://en.wikipedia.org/?curid=40409788", "score": 0.40713822841644287, "chunk_id": "30", "preview": "typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limitin"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:24.871812Z"}
{"id": "q8", "question": "What is a transformer in machine learning?", "answer": "A transformer in machine learning is an artificial neural network architecture based on the multi-head attention mechanism. It converts text into numerical tokens, which are then represented as vectors via word embeddings. Each token is contextualized with other tokens in the input through parallel multi-head attention, allowing important tokens to be emphasized and less important ones diminished. Unlike earlier recurrent neural networks (RNNs), transformers do not use recurrent units, enabling faster training. They were introduced in the 2017 paper \"Attention Is All You Need\" and have since been widely used in natural language processing, computer vision, and other fields, forming the basis for models like GPT and BERT [1][2].", "confidence": {"label": "High", "value": 0.85, "best": 0.3699333667755127, "worst": 0.4809539318084717, "spread": 0.11102056503295898}, "sources": [{"n": 1, "title": "Transformer (deep learning)", "url": "https://en.wikipedia.org/?curid=61603971", "score": 0.3699333667755127, "chunk_id": "0", "preview": "In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations "}, {"n": 2, "title": "Neural network (machine learning)", "url": "https://en.wikipedia.org/?curid=21523", "score": 0.464857816696167, "chunk_id": "10", "preview": "During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need. It requi"}, {"n": 3, "title": "Machine learning", "url": "https://en.wikipedia.org/?curid=233488", "score": 0.4809539318084717, "chunk_id": "17", "preview": "== Models == A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. Durin"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:29.477755Z"}
{"id": "q9", "question": "What is k-means clustering?", "answer": "K-means clustering is a method of vector quantization that partitions n observations into k clusters, where each observation belongs to the cluster with the nearest mean (cluster centroid). It aims to minimize the within-cluster variances (squared Euclidean distances), resulting in a partitioning of the data space into Voronoi cells. The algorithm iteratively refines cluster centers to converge to a local optimum, grouping data points into clusters of comparable spatial extent [1].", "confidence": {"label": "High", "value": 0.85, "best": 0.26281607151031494, "worst": 0.26281607151031494, "spread": 0.0}, "sources": [{"n": 1, "title": "K-means clustering", "url": "https://en.wikipedia.org/?curid=1860407", "score": 0.26281607151031494, "chunk_id": "0", "preview": "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:32.648683Z"}
{"id": "q10", "question": "What is the capital of France?", "answer": "I don't have that information in my sources.", "confidence": {"label": "Low", "value": 0.45, "best": 0.85515296459198, "worst": 0.8903846740722656, "spread": 0.035231709480285645}, "sources": [{"n": 1, "title": "F-score", "url": "https://en.wikipedia.org/?curid=4011785", "score": 0.85515296459198, "chunk_id": "2", "preview": "\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}}  With precision = TP / (TP + FP) and recall = TP / (TP + FN), it follows that the numerator of F1 is the sum of their n"}, {"n": 2, "title": "Transformer (deep learning)", "url": "https://en.wikipedia.org/?curid=61603971", "score": 0.8731182217597961, "chunk_id": "59", "preview": "==== Random Feature Attention ==== Random Feature Attention (2021) uses Fourier random features:                        φ         (         x         )         =                   "}, {"n": 3, "title": "Principal component analysis", "url": "https://en.wikipedia.org/?curid=76340", "score": 0.8820353150367737, "chunk_id": "56", "preview": "information contained in [several] macroeconomic variables into a more manageable data set, which can then [be used] for analysis.\" Here, the resulting factors are linked to e.g. i"}, {"n": 4, "title": "Receiver operating characteristic", "url": "https://en.wikipedia.org/?curid=922505", "score": 0.8863251805305481, "chunk_id": "9", "preview": "→         y         (         x         )                 {\\displaystyle \\operatorname {TPR} (T):T\\to y(x)}  FPR         ⁡         (         T         )         :         T        "}, {"n": 5, "title": "Decision tree learning", "url": "https://en.wikipedia.org/?curid=577003", "score": 0.887172281742096, "chunk_id": "27", "preview": "To find                         φ         (         s         ∣         t         )                 {\\displaystyle \\varphi (s\\mid t)}     of the feature savings, we need to note th"}, {"n": 6, "title": "Logistic regression", "url": "https://en.wikipedia.org/?curid=226631", "score": 0.8903846740722656, "chunk_id": "77", "preview": "mathbf {X} _{i}}e^{\\mathbf {C} \\cdot \\mathbf {X} _{i}}}{e^{{\\boldsymbol {\\beta }}_{0}\\cdot \\mathbf {X} _{i}}e^{\\mathbf {C} \\cdot \\mathbf {X} _{i}}+e^{{\\boldsymbol {\\beta }}_{1}\\cdo"}], "ok": true, "notes": [], "timestamp": "2026-02-24T17:53:35.163265Z"}
